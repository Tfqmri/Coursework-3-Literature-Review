@inproceedings{Hu2022lora,
  author    = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {Proceedings of the Tenth International Conference on Learning Representations (ICLR)},
  year      = {2022},
  url       = {https://api.semanticscholar.org/CorpusID:235458009},
  keywords  = {type:empirical, method:LoRA, model:GPT, task:classification, evaluation:benchmark, data:text, framework:PyTorch}
}

@inproceedings{Houlsby2019adapter,
  author    = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  title     = {Parameter-Efficient Transfer Learning for NLP with Adapter Modules},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year      = {2019},
  url       = {https://proceedings.mlr.press/v97/houlsby19a.html},
  keywords  = {type:empirical, method:adapter, model:BERT, task:sentiment analysis, evaluation:benchmark, data:text, framework:TensorFlow}
}

@inproceedings{Zaken2022bitfit,
  author    = {Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
  title     = {BitFit: Simple Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language Models},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2022},
  url       = {https://aclanthology.org/2022.acl-short.1/},
  keywords  = {type:empirical, method:BitFit, model:RoBERTa, task:classification, evaluation:benchmark, data:text, framework:PyTorch}
}

@inproceedings{Li2021prefix,
  author    = {Xiang Lisa Li and Percy Liang},
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2021},
  url       = {https://aclanthology.org/2021.acl-long.353/},
  keywords  = {type:empirical, method:prefix-tuning, model:T5, task:summarization, evaluation:benchmark, data:text, framework:TensorFlow}
}

@article{Dettmers2023qlora,
  author    = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  title     = {QLoRA: Quantization-aware Low-Rank Adapter for LLMs},
  journal   = {arXiv preprint arXiv:2305.14314},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.14314},
  keywords  = {type:empirical, method:QLoRA, model:LLaMA, task:question answering, evaluation:benchmark, data:text, framework:HuggingFace}
}

@article{Fu2022tfew,
  author    = {Yao Fu and Hao Peng and Ashish Sabharwal and Noah A. Smith},
  title     = {T-Few: Parameter-Efficient Few-Shot Learning with T5},
  journal   = {arXiv preprint arXiv:2205.05638},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.05638},
  keywords  = {type:empirical, method:adapter, model:T5, task:few-shot learning, evaluation:benchmark, data:text, framework:PyTorch}
}

@inproceedings{Li2023adalora,
  author    = {Yitong Li and et al.},
  title     = {AdaLoRA: Enhancing Low-Rank Adaptation with Dynamic Rank Allocation},
  booktitle = {Proceedings of the Eleventh International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://openreview.net/forum?id=lq62uWRJjiY},
  keywords  = {type:empirical, method:AdaLoRA, model:LLaMA, task:text generation, evaluation:benchmark, data:text, framework:PyTorch}
}


@article{Liu2025survey,
  author    = {Xiaotao Liu and et al.},
  title     = {A Survey of Parameter-Efficient Fine-Tuning Methods for LLMs},
  journal   = {Artificial Intelligence Review},
  year      = {2025},
  url       = {https://link.springer.com/article/10.1007/s10462-025-11236-4},
  keywords  = {type:survey, method:multi, model:multi, task:multi, evaluation:comparison, data:text, framework:various}
}

@inproceedings{Rotman2021compacter,
  author    = {Noam Rotman and Roi Reichart},
  title     = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.243/},
  keywords  = {type:empirical, method:Compacter, model:BERT, task:classification, evaluation:benchmark, data:text, framework:PyTorch}
}

@article{Du2023llamaadapter,
  author    = {Zhengxiao Du and Yujie Fan and et al.},
  title     = {LLaMA-Adapters: Parameter-Efficient Fine-Tuning for LLaMA Models},
  journal   = {arXiv preprint arXiv:2303.16199},
  year      = {2023},
  url       = {https://arxiv.org/abs/2303.16199},
  keywords  = {type:empirical, method:adapter, model:LLaMA, task:dialogue, evaluation:benchmark, data:text, framework:HuggingFace}
}
