define({ entries : {
    "Dettmers2023qlora": {
        "author": "Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer",
        "journal": "arXiv preprint arXiv:2305.14314",
        "keywords": "type:empirical, method:QLoRA, model:LLaMA, task:question answering, evaluation:benchmark, data:text, framework:HuggingFace",
        "title": "QLoRA: Quantization-aware Low-Rank Adapter for LLMs",
        "type": "article",
        "url": "https://arxiv.org/abs/2305.14314",
        "year": "2023"
    },
    "Du2023llamaadapter": {
        "author": "Zhengxiao Du and Yujie Fan and et al.",
        "journal": "arXiv preprint arXiv:2303.16199",
        "keywords": "type:empirical, method:adapter, model:LLaMA, task:dialogue, evaluation:benchmark, data:text, framework:HuggingFace",
        "title": "LLaMA-Adapters: Parameter-Efficient Fine-Tuning for LLaMA Models",
        "type": "article",
        "url": "https://arxiv.org/abs/2303.16199",
        "year": "2023"
    },
    "Fu2022tfew": {
        "author": "Yao Fu and Hao Peng and Ashish Sabharwal and Noah A. Smith",
        "journal": "arXiv preprint arXiv:2205.05638",
        "keywords": "type:empirical, method:adapter, model:T5, task:few-shot learning, evaluation:benchmark, data:text, framework:PyTorch",
        "title": "T-Few: Parameter-Efficient Few-Shot Learning with T5",
        "type": "article",
        "url": "https://arxiv.org/abs/2205.05638",
        "year": "2022"
    },
    "Houlsby2019adapter": {
        "author": "Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly",
        "booktitle": "Proceedings of the 36th International Conference on Machine Learning (ICML)",
        "keywords": "type:empirical, method:adapter, model:BERT, task:sentiment analysis, evaluation:benchmark, data:text, framework:TensorFlow",
        "title": "Parameter-Efficient Transfer Learning for NLP with Adapter Modules",
        "type": "inproceedings",
        "url": "https://proceedings.mlr.press/v97/houlsby19a.html",
        "year": "2019"
    },
    "Hu2022lora": {
        "author": "Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "booktitle": "Proceedings of the Tenth International Conference on Learning Representations (ICLR)",
        "keywords": "type:empirical, method:LoRA, model:GPT, task:classification, evaluation:benchmark, data:text, framework:PyTorch",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "type": "inproceedings",
        "url": "https://api.semanticscholar.org/CorpusID:235458009",
        "year": "2022"
    },
    "Li2021prefix": {
        "author": "Xiang Lisa Li and Percy Liang",
        "booktitle": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)",
        "keywords": "type:empirical, method:prefix-tuning, model:T5, task:summarization, evaluation:benchmark, data:text, framework:TensorFlow",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2021.acl-long.353/",
        "year": "2021"
    },
    "Li2023adalora": {
        "author": "Yitong Li and et al.",
        "booktitle": "Proceedings of the Eleventh International Conference on Learning Representations (ICLR)",
        "keywords": "type:empirical, method:AdaLoRA, model:LLaMA, task:text generation, evaluation:benchmark, data:text, framework:PyTorch",
        "title": "AdaLoRA: Enhancing Low-Rank Adaptation with Dynamic Rank Allocation",
        "type": "inproceedings",
        "url": "https://openreview.net/forum?id",
        "year": "2023"
    },
    "Liu2025survey": {
        "author": "Xiaotao Liu and et al.",
        "journal": "Artificial Intelligence Review",
        "keywords": "type:survey, method:multi, model:multi, task:multi, evaluation:comparison, data:text, framework:various",
        "title": "A Survey of Parameter-Efficient Fine-Tuning Methods for LLMs",
        "type": "article",
        "url": "https://link.springer.com/article/10.1007/s10462-025-11236-4",
        "year": "2025"
    },
    "Rotman2021compacter": {
        "author": "Noam Rotman and Roi Reichart",
        "booktitle": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
        "keywords": "type:empirical, method:Compacter, model:BERT, task:classification, evaluation:benchmark, data:text, framework:PyTorch",
        "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2021.emnlp-main.243/",
        "year": "2021"
    },
    "Zaken2022bitfit": {
        "author": "Ben Zaken and Shauli Ravfogel and Yoav Goldberg",
        "booktitle": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)",
        "keywords": "type:empirical, method:BitFit, model:RoBERTa, task:classification, evaluation:benchmark, data:text, framework:PyTorch",
        "title": "BitFit: Simple Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language Models",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2022.acl-short.1/",
        "year": "2022"
    }
}});